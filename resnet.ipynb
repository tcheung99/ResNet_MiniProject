{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcheung99/ResNet_MiniProject/blob/master/resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlusWoelduH8",
        "colab_type": "code",
        "outputId": "126e2e3b-0e91-4a68-e37b-a7462a6224f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tyuiupzld805",
        "colab_type": "code",
        "outputId": "9db83bf2-24bc-4726-c70b-be1b9f0aab20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCDJrBtHj5-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "from six.moves import urllib\n",
        "import sys\n",
        "import numpy as np\n",
        "import _pickle as cPickle\n",
        "import os\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq5rU4AQFVlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KaFIpmHWgDV",
        "colab_type": "text"
      },
      "source": [
        "**Formulation** \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Based on He et. al's *Deep Residual Learning for Image Recognition*.\n",
        "\n",
        "General:\n",
        "\n",
        "*   Full pre-activation residual block\n",
        "*   No bottleneck residual blocks\n",
        "*   Perform 1x1 convolution insteaad of padding input (projection shortcut) for matching output size \n",
        "\n",
        "\n",
        "*   1st layer is 3x3 convolutions \n",
        "*   Stack of 6n layers with 3x3 on feature maps of size {32,16,8} respectively (n = number of residual blocks) \n",
        "\n",
        "\n",
        "*   Ends with global average pooling, a FC layer, and softmax\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "CIFAR-10:\n",
        "\n",
        "*   Image size = 32x32\n",
        "*   Label Dimension = 10\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2_ayBRhQ_D0",
        "colab_type": "text"
      },
      "source": [
        "# Loading training and test data for the CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEGJWJAN0-yA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6d92b822-6d1a-41d4-d54d-f12e3a94b1b4"
      },
      "source": [
        "data_dir = 'cifar10_data'\n",
        "full_data_dir = data_dir + '/cifar-10-batches-py/data_batch_'\n",
        "valid_dir = data_dir + '/cifar-10-batches-py/test_batch'\n",
        "URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
        "\n",
        "img_width = 32\n",
        "img_height = 32\n",
        "img_depth = 3\n",
        "\n",
        "num_train_batches = 5 \n",
        "num_epochs = 10000 * num_train_batches\n",
        "num_classes = 10\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "filename = 'cifar-10-python.tar.gz'\n",
        "filepath = os.path.join(data_dir, filename)\n",
        "\n",
        "if not os.path.exists(filepath):\n",
        "    def _progress(count, block_size, total_size):\n",
        "        sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename, float(count * block_size)/ float(total_size) * 100.0))\n",
        "        sys.stdout.flush()\n",
        "    filepath, _ = urllib.request.urlretrieve(URL, filepath, _progress)\n",
        "    print()\n",
        "    statinfo = os.stat(filepath)\n",
        "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
        "    tarfile.open(filepath, 'r:gz').extractall(data_dir)\n",
        "\n",
        "def read_one_batch(path, use_random_label):\n",
        "    '''\n",
        "    The training data contains five data batches in total. The validation data has only one\n",
        "    batch. This function takes the directory of one batch of data and returns the images and\n",
        "    corresponding labels as numpy arrays\n",
        "    :param path: the directory of one batch of data\n",
        "    :param is_random_label: do you want to use random labels?\n",
        "    :return: image numpy arrays and label numpy arrays\n",
        "    '''\n",
        "    infile = open(path, 'rb')\n",
        "    new_dict = cPickle.load(infile, encoding='latin1')\n",
        "    infile.close()\n",
        "\n",
        "    image = new_dict['data']\n",
        "    if use_random_label: # Add random labels \n",
        "        labels = np.random.randint(low=0, high=10, size=10000)\n",
        "        label = np.array(labels)\n",
        "    else:\n",
        "        label = np.array(new_dict['labels'])\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "\n",
        "def read_all(all_paths, shuffle=True, use_random_label = False):\n",
        "    \"\"\"\n",
        "    This function reads all training or validation data, shuffles them if needed, and returns the\n",
        "    images and the corresponding labels as numpy arrays\n",
        "    :param address_list: a list of paths of cPickle files\n",
        "    :return: concatenated numpy array of data and labels. Data are in 4D arrays: [num_images,\n",
        "    image_height, image_width, image_depth] and labels are in 1D arrays: [num_images]\n",
        "    \"\"\"\n",
        "    data = np.array([]).reshape([0, img_width * img_height * img_depth])\n",
        "    label = np.array([])\n",
        "\n",
        "    for path in all_paths:\n",
        "        batch_data, batch_label = read_one_batch(path, use_random_label)\n",
        "        # np.concatenate concatenates along the 0-axis\n",
        "        data = np.concatenate((data, batch_data))\n",
        "        label = np.concatenate((label, batch_label))\n",
        "\n",
        "    num_data = len(label)\n",
        "\n",
        "    data = data.reshape((num_data, img_height * img_width, img_depth), order='F')\n",
        "    data = data.reshape((num_data, img_height, img_width, img_depth))\n",
        "\n",
        "    if shuffle:\n",
        "        print('Shuffling')\n",
        "        order = np.random.permutation(num_data)\n",
        "        data = data[order, ...]\n",
        "        label = label[order]\n",
        "\n",
        "    data = data.astype(np.float32)\n",
        "    return data, label\n",
        "\n",
        "def random_crop_flip(batch_images, padding_size):\n",
        "    '''\n",
        "    Helper to random crop and random flip a batch of images\n",
        "    :param padding_size: int. how many layers of 0 padding was added to each side\n",
        "    :param batch_data: a 4D batch array\n",
        "    :return: randomly cropped and flipped image\n",
        "    '''\n",
        "    cropped_batch = np.zeros(len(batch_images) * img_height * img_width * img_depth).reshape(len(batch_images), img_height, img_width, img_depth)\n",
        "    flip_prop = np.random.randint(low=0, high=2)\n",
        "\n",
        "    for i in range(len(batch_images)):\n",
        "        x_offset = np.random.randint(low=0, high=2 * padding_size, size=1)[0]\n",
        "        y_offset = np.random.randint(low=0, high=2 * padding_size, size=1)[0]\n",
        "        cropped_batch[i, ...] = batch_images[i, ...][x_offset:x_offset+img_height,y_offset:y_offset+img_width, :]\n",
        "\n",
        "    if flip_prop == 0:\n",
        "        cropped_batch[i, ...] = cv2.flip(cropped_batch[i, ...], 1)\n",
        "\n",
        "    return cropped_batch"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Downloading cifar-10-python.tar.gz 100.0%\n",
            "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QMIP5OSPQX",
        "colab_type": "text"
      },
      "source": [
        "Define Training Hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCm_jg9S38D2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS = tf.app.flags.FLAGS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCdO1kyGSTaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE args: flagname, defaultvalue, docstring\n",
        "# tf.app.flags.DEFINE_integer('max_train_steps', 70000, 'Number of steps for training')\n",
        "\n",
        "# tf.app.flags.DEFINE_float('lr_init', 0.1, 'Initial learning rate')\n",
        "# tf.app.flags.DEFINE_float('lr_decay_fact', 0.1, 'Factor to decay the learning rate by')\n",
        "\n",
        "# tf.app.flags.DEFINE_integer('num_residual_block', 2, 'Number of residual blocks')\n",
        "# tf.app.flags.DEFINE_float('weight_decay', 0.00025, 'l2 regularization')\n",
        "\n",
        "\n",
        "## The following flags are related to save paths, tensorboard outputs and screen outputs\n",
        "\n",
        "tf.app.flags.DEFINE_string('version', 'test_3', '''A version number defining the directory to save\n",
        "logs and checkpoints''')\n",
        "tf.app.flags.DEFINE_integer('report_freq', 400, '''Steps takes to output errors on the screen\n",
        "and write summaries''')\n",
        "tf.app.flags.DEFINE_float('train_ema_decay', 0.95, '''The decay factor of the train error's\n",
        "moving average shown on tensorboard''')\n",
        "\n",
        "\n",
        "## The following flags define hyper-parameters regards training\n",
        "\n",
        "tf.app.flags.DEFINE_integer('train_steps', 20000, 'Number of steps for training')\n",
        "tf.app.flags.DEFINE_boolean('is_full_validation', False, '''Validation w/ full validation set or a random batch''')\n",
        "tf.app.flags.DEFINE_integer('train_batch_size', 128, 'Train batch size')\n",
        "tf.app.flags.DEFINE_integer('validation_batch_size', 250, 'Validation batch size')\n",
        "tf.app.flags.DEFINE_integer('test_batch_size', 125, 'Test batch size')\n",
        "\n",
        "tf.app.flags.DEFINE_float('init_lr', 0.1, 'Initial learning rate')\n",
        "tf.app.flags.DEFINE_float('lr_decay_factor', 0.1, 'Factor to decay the learning rate')\n",
        "tf.app.flags.DEFINE_integer('decay_step0', 11000, 'First step to decay the learning rate')\n",
        "tf.app.flags.DEFINE_integer('decay_step1', 16000, 'Second step to decay the learning rate')\n",
        "\n",
        "\n",
        "## The following flags define hyper-parameters modifying the training network\n",
        "\n",
        "tf.app.flags.DEFINE_integer('num_residual_blocks', 2, 'Number of residual blocks')\n",
        "# tf.app.flags.DEFINE_float('weight_decay', 0.0002, '''scale for l2 regularization''')\n",
        "tf.app.flags.DEFINE_float('weight_decay', 0.0001, 'Scale for l2 regularization')\n",
        "\n",
        "\n",
        "## The following flags are related to data-augmentation\n",
        "\n",
        "tf.app.flags.DEFINE_integer('padding_size', 2, '''In data augmentation, layers of zero padding on each side of the image''')\n",
        "\n",
        "\n",
        "## If you want to load a checkpoint and continue training\n",
        "\n",
        "tf.app.flags.DEFINE_string('ckpt_path', 'cache/logs_repeat20/model.ckpt-100000', '''Checkpoint\n",
        "directory to restore''')\n",
        "tf.app.flags.DEFINE_boolean('is_use_ckpt', False, '''Whether to load a checkpoint and continue\n",
        "training''')\n",
        "\n",
        "tf.app.flags.DEFINE_string('test_ckpt_path', 'model_110.ckpt-79999', '''Checkpoint\n",
        "directory to restore''')\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "train_dir = 'logs_' + FLAGS.version + '/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfgPeq6XRFqZ",
        "colab_type": "text"
      },
      "source": [
        "# Define ResNet functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_idz0EPXudU",
        "colab_type": "text"
      },
      "source": [
        "A residual block is when activation of a layer is \"fast-forwarded\" to a deeper layer \n",
        "\n",
        "*   Abstract to passing input through BN, relu, conv \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-i5ybxpWkR9",
        "colab_type": "code",
        "outputId": "1290f2c9-043a-458e-94b4-45879924bf1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "def make_var(name,shape,initializer=tf.contrib.layers.xavier_initializer(),is_fc_layer=False):\n",
        "  # initializer = tf.contrib.layers.xavier_initializer()\n",
        "  regularizer = tf.contrib.layers.l2_regularizer(scale=FLAGS.weight_decay)\n",
        "  new_var = tf.get_variable(name, shape=shape, initializer=initializer, regularizer=regularizer)\n",
        "  return new_var"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtHby_0fwuFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bn_layer(input_layer, input_depth):\n",
        "  # Batch norm layer \n",
        "  mean, var = tf.nn.moments(input_layer, axes=[0,1,2])\n",
        "  beta = tf.get_variable('beta', input_depth, tf.float32, tf.constant_initializer(0.0,tf.float32))\n",
        "  gamma = tf.get_variable('gamma', input_depth, tf.float32, tf.constant_initializer(1.0,tf.float32))\n",
        "  BN_layer = tf.nn.batch_norm_with_global_normalization(input_layer, mean, var, beta,gamma, 0.001)\n",
        "  return BN_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMl1S0Khm1f5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_layer(input_layer, input_depth, filter_shape,stride):\n",
        "  #conv(relu(bn(input_layer)))\n",
        "\n",
        "  BN_layer = bn_layer(input_layer, input_depth)\n",
        "\n",
        "  # Apply ReLu \n",
        "  ReLu_layer = tf.nn.relu(BN_layer)\n",
        "\n",
        "  # Conv layer \n",
        "  filter = make_var(name='conv',shape=filter_shape)\n",
        "  conv_layer = tf.nn.conv2d(ReLu_layer, filter=filter,strides=[1,stride,stride,1],padding='SAME')\n",
        "\n",
        "  return conv_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geiMvRkzqgth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def res_block(input_layer, output_depth, first_block=False):\n",
        "  input_depth = input_layer.get_shape().as_list()[-1]\n",
        "  if input_depth == output_depth:\n",
        "    project = False \n",
        "    stride = 1 \n",
        "  elif input_depth != output_depth:\n",
        "    project = True \n",
        "    stride = 2 \n",
        "  with tf.variable_scope('conv1_in'):\n",
        "    if first_block:\n",
        "      filter = make_var('conv',[3,3,input_depth,output_depth])    \n",
        "      conv1 = tf.nn.conv2d(input_layer,filter=filter,strides=[1,1,1,1],padding='SAME')\n",
        "    elif not first_block:\n",
        "      conv1 = conv_layer(input_layer,input_depth, [3,3,input_depth,output_depth],stride)\n",
        "\n",
        "  with tf.variable_scope('conv2_in'):\n",
        "    conv2 = conv_layer(conv1, conv1.get_shape().as_list()[-1], [3,3,output_depth,output_depth],1)\n",
        "\n",
        "  if project == True:\n",
        "    # 1x1 conv shortcut projection \n",
        "    # mod_input = tf.nn.conv2d(input_layer, [1,1,input_depth,output_depth], stride)\n",
        "\n",
        "    mod_input = conv_layer(input_layer, input_depth, [1,1,input_depth,output_depth],2)\n",
        "\n",
        "    # pooled_input = tf.nn.avg_pool(input_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "    # padded_input = tf.pad(pooled_input, [[0, 0], [0, 0], [0, 0], [input_depth // 2,input_depth // 2]])                                                          \n",
        "    # mod_input = padded_input\n",
        "  elif project == False:\n",
        "    mod_input = input_layer\n",
        "\n",
        "  result = conv2 + mod_input\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTzF-F2hxB0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_layer(input_layer, dimensions):\n",
        "  input_depth = input_layer.get_shape().as_list()[-1]\n",
        "  BN_layer = bn_layer(input_layer, input_depth)\n",
        "  ReLu_layer = tf.nn.relu(BN_layer)\n",
        "  pool_layer = tf.reduce_mean(ReLu_layer, [1, 2])\n",
        "\n",
        "  assert pool_layer.get_shape().as_list()[-1:] == [64]\n",
        "  ##\n",
        "  fc_w = make_var(name='fc_weights', shape=[input_depth, dimensions],initializer=tf.uniform_unit_scaling_initializer(factor=1.0), is_fc_layer=True)\n",
        "  fc_b = make_var(name='fc_bias', shape=[dimensions], initializer=tf.zeros_initializer())\n",
        "  fc_h = tf.nn.softmax(tf.matmul(pool_layer, fc_w) + fc_b)\n",
        "  \n",
        "  return fc_h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QAy4l5ixNTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_summary(t):\n",
        "  name = t.op.name\n",
        "  tf.summary.histogram(name+'/activations', t)\n",
        "  tf.summary.scalar(name + '/sparsity', tf.nn.zero_fraction(t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1HMtdiCumvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet(input_tensor,n,reuse):\n",
        "\n",
        "  all_layers = list()\n",
        "\n",
        "  # Op on input tensor\n",
        "  with tf.variable_scope('conv0',reuse=reuse):\n",
        "    filter = make_var(name='conv',shape=[3,3,3,16])\n",
        "    conv_layer = tf.nn.conv2d(input_tensor, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
        "    BN_layer = bn_layer(conv_layer, 16)\n",
        "    conv0 = tf.nn.relu(BN_layer)\n",
        "    tensor_summary(conv0)\n",
        "    all_layers.append(conv0)\n",
        "\n",
        "  for i in range(n):\n",
        "    with tf.variable_scope('conv1_%d' %i, reuse=reuse):\n",
        "      if i == 0:\n",
        "          conv1 = res_block(all_layers[-1], 16, first_block=True)\n",
        "      else:\n",
        "          conv1 = res_block(all_layers[-1], 16)\n",
        "      tensor_summary(conv1)\n",
        "      all_layers.append(conv1)\n",
        "\n",
        "  for i in range(n):\n",
        "    with tf.variable_scope('conv2_%d' %i, reuse=reuse):\n",
        "      conv2 = res_block(all_layers[-1], 32)\n",
        "      tensor_summary(conv2)\n",
        "      all_layers.append(conv2)\n",
        "\n",
        "  for i in range(n):\n",
        "    with tf.variable_scope('conv3_%d' %i, reuse=reuse):\n",
        "      conv3 = res_block(all_layers[-1], 64)\n",
        "      tensor_summary(conv3)\n",
        "      all_layers.append(conv3)\n",
        "    assert conv3.get_shape().as_list()[1:] == [8, 8, 64]\n",
        "\n",
        "  with tf.variable_scope('fc', reuse=reuse):\n",
        "    fc = fc_layer(all_layers[-1],10)\n",
        "    all_layers.append(fc)\n",
        "\n",
        "  return all_layers[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GakRQ8w0A2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_graph(train_dir='logs'):\n",
        "  input_tensor = tf.constant(np.ones([128, 32, 32, 3]), dtype=tf.float32)\n",
        "  output = resnet(input_tensor, reuse=False, n=2)\n",
        "  init = tf.initialize_all_variables()\n",
        "  session = tf.Session()\n",
        "  session.run(init)\n",
        "  summary_writer = tf.train.SummaryWriter(train_dir, sess.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rdCC6ZlSC7j",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5gv0kHx1A-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Train_ResNet(object):\n",
        "  def __init__(self):\n",
        "    # Initialize placeholders\n",
        "    self.lr_ph = tf.placeholder(dtype=tf.float32, shape=[])\n",
        "    self.image_ph = tf.placeholder(dtype=tf.float32, shape=[FLAGS.train_batch_size, img_height,img_width, img_depth])\n",
        "    self.label_ph = tf.placeholder(dtype=tf.int32, shape=[FLAGS.train_batch_size])\n",
        "    self.valid_image_ph = tf.placeholder(dtype=tf.float32, shape=[FLAGS.validation_batch_size, img_height, img_width, img_depth])\n",
        "    self.valid_label_ph = tf.placeholder(dtype=tf.int32, shape=[FLAGS.validation_batch_size])\n",
        "\n",
        "  def build_train_valid_graph(self):\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "    validation_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    logits = resnet(self.image_ph, FLAGS.num_residual_blocks, reuse=False)\n",
        "    \n",
        "    # Reuse weight from training, reuse=True \n",
        "    valid_logits = resnet(self.valid_image_ph, FLAGS.num_residual_blocks, reuse=True)\n",
        "\n",
        "    # Training loss \n",
        "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    loss = self.loss(logits, self.label_ph)\n",
        "    self.all_loss = tf.add_n([loss] + reg_losses)\n",
        "\n",
        "    predictions = tf.nn.softmax(logits)\n",
        "    self.train_top1_error, self.train_acc = self.top_k_error(predictions, self.label_ph, 1)\n",
        "\n",
        "    # Validation loss\n",
        "    self.valid_loss = self.loss(valid_logits, self.valid_label_ph)\n",
        "    vali_predictions = tf.nn.softmax(valid_logits)\n",
        "    self.vali_top1_error, self.val_acc = self.top_k_error(vali_predictions, self.valid_label_ph, 1)\n",
        "\n",
        "    self.train_op, self.train_ema_op = self.train_operation(global_step, self.all_loss,self.train_top1_error,self.train_acc)\n",
        "    self.val_op = self.validation_operation(validation_step, self.vali_top1_error, self.valid_loss,self.val_acc)\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    train_paths = list() \n",
        "    for i in range(1, num_train_batches+1):\n",
        "        train_paths.append(full_data_dir + str(i))\n",
        "    all_data, all_labels = read_all(train_paths, use_random_label=False)     \n",
        "    \n",
        "    pad_width = ((0, 0), (FLAGS.padding_size, FLAGS.padding_size), (FLAGS.padding_size, FLAGS.padding_size), (0, 0))\n",
        "    all_data = np.pad(all_data, pad_width=pad_width, mode='constant', constant_values=0)\n",
        "\n",
        "    valid_data, valid_labels = read_all([valid_dir],use_random_label=False)\n",
        "\n",
        "    # Build train and validation graph\n",
        "    self.build_train_valid_graph()\n",
        "\n",
        "    # Saver to save checkpoints \n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    summary_op = tf.summary.merge_all()\n",
        "    init = tf.initialize_all_variables()\n",
        "    sess = tf.Session()\n",
        "\n",
        "    # Loading from a checkpoint\n",
        "    if FLAGS.is_use_ckpt is True:\n",
        "      saver.restore(sess, FLAGS.ckpt_path)\n",
        "      print ('Restored from checkpoint')\n",
        "    else:\n",
        "      sess.run(init)\n",
        "\n",
        "    summary_writer = tf.summary.FileWriter(train_dir, sess.graph)\n",
        "\n",
        "    step_list = []\n",
        "    train_error_list = []\n",
        "    val_error_list = []\n",
        "\n",
        "    print ('Training')\n",
        "    print ('-----------------------------------')\n",
        "\n",
        "    for step in range(FLAGS.train_steps):\n",
        "      train_offset = np.random.choice(num_epochs - FLAGS.train_batch_size, 1)[0]\n",
        "      batch_data = all_data[train_offset:train_offset + FLAGS.train_batch_size, ...]\n",
        "      train_batch_data = random_crop_flip(batch_data, padding_size=FLAGS.padding_size)\n",
        "      train_batch_labels = all_labels[train_offset:train_offset+FLAGS.train_batch_size]\n",
        "\n",
        "      val_offset = np.random.choice(10000 - FLAGS.validation_batch_size, 1)[0]\n",
        "      validation_batch_data = valid_data[val_offset:val_offset+FLAGS.validation_batch_size, ...]\n",
        "      validation_batch_labels = valid_labels[val_offset:val_offset+FLAGS.validation_batch_size]\n",
        "\n",
        "      # Want to validate once before training. You may check the theoretical validation\n",
        "      # loss first\n",
        "      if step % FLAGS.report_freq == 0:\n",
        "        if FLAGS.is_full_validation is True:\n",
        "          validation_loss_value, validation_error_value = self.full_validation(\n",
        "                                        loss=self.valid_loss,\n",
        "                                        top1_error=self.vali_top1_error, vali_data=valid_data,\n",
        "                                        vali_labels=valid_labels, session=sess,\n",
        "                                        batch_data=train_batch_data, batch_label=train_batch_labels)\n",
        "          vali_summ = tf.Summary()\n",
        "          vali_summ.value.add(tag='full_validation_error',\n",
        "                              simple_value=validation_error_value.astype(np.float))\n",
        "          summary_writer.add_summary(vali_summ, step)\n",
        "          summary_writer.flush()\n",
        "\n",
        "        else:\n",
        "          _, validation_error_value, validation_loss_value = sess.run(\n",
        "                                        [self.val_op,self.vali_top1_error,self.valid_loss],\n",
        "                                        {self.image_ph: train_batch_data,\n",
        "                                          self.label_ph: train_batch_labels,\n",
        "                                          self.valid_image_ph: validation_batch_data,\n",
        "                                          self.valid_label_ph: validation_batch_labels,\n",
        "                                          self.lr_ph: FLAGS.init_lr})\n",
        "\n",
        "          val_error_list.append(validation_error_value)\n",
        "\n",
        "      start_time = time.time()\n",
        "      _, _, train_loss_value, train_error_value = sess.run(\n",
        "                                          [self.train_op, self.train_ema_op,self.all_loss, self.train_top1_error],\n",
        "                                          {self.image_ph: train_batch_data,\n",
        "                                          self.label_ph: train_batch_labels,\n",
        "                                          self.valid_image_ph: validation_batch_data,\n",
        "                                          self.valid_label_ph: validation_batch_labels,\n",
        "                                          self.lr_ph: FLAGS.init_lr})\n",
        "      duration = time.time() - start_time\n",
        "\n",
        "      if step % FLAGS.report_freq == 0:\n",
        "        summary_str = sess.run(summary_op, {self.image_ph: train_batch_data,\n",
        "                                            self.label_ph: train_batch_labels,\n",
        "                                            self.valid_image_ph: validation_batch_data,\n",
        "                                            self.valid_label_ph: validation_batch_labels,\n",
        "                                            self.lr_ph: FLAGS.init_lr})\n",
        "        summary_writer.add_summary(summary_str, step)\n",
        "\n",
        "        num_examples_per_step = FLAGS.train_batch_size\n",
        "        examples_per_sec = num_examples_per_step / duration\n",
        "        sec_per_batch = float(duration)\n",
        "\n",
        "        format_str = ('%s: step %d, loss = %.4f (%.1f examples/sec; %.3f ' 'sec/batch)')\n",
        "        print (format_str % (datetime.now(), step, train_loss_value, examples_per_sec,sec_per_batch))\n",
        "        print ('Train top1 err = ', train_error_value)\n",
        "        print ('Valid top1 err = %.4f' % validation_error_value)\n",
        "        print ('Valid loss = ', validation_loss_value)\n",
        "        print ('-----------------------------------')\n",
        "\n",
        "        step_list.append(step)\n",
        "        train_error_list.append(train_error_value)\n",
        "\n",
        "      if step == FLAGS.decay_step0 or step == FLAGS.decay_step1:\n",
        "        FLAGS.init_lr = 0.1 * FLAGS.init_lr\n",
        "        print ('Learning rate after decay ', FLAGS.init_lr)\n",
        "\n",
        "      # Save checkpoints every 10000 steps\n",
        "      if step % 10000 == 0 or (step + 1) == FLAGS.train_steps:\n",
        "        checkpoint_path = os.path.join(train_dir, 'model.ckpt')\n",
        "        saver.save(sess, checkpoint_path, global_step=step)\n",
        "\n",
        "        df = pd.DataFrame(data={'step':step_list, 'train_error':train_error_list,'validation_error': val_error_list})\n",
        "        df.to_csv(train_dir + FLAGS.version + '_error.csv')\n",
        "\n",
        "\n",
        "  def test(self, test_image_array):\n",
        "    num_test_images = len(test_image_array)\n",
        "    num_batches = num_test_images // FLAGS.test_batch_size\n",
        "    remain_images = num_test_images % FLAGS.test_batch_size\n",
        "    print ('%i test batches in total...' %num_batches)\n",
        "\n",
        "    # Create the test image and labels placeholders\n",
        "    self.test_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[FLAGS.test_batch_size, img_height, img_width, img_depth])\n",
        "\n",
        "    # Build the test graph\n",
        "    logits = resnet(self.test_image_placeholder, FLAGS.num_residual_blocks, reuse=False)\n",
        "    predictions = tf.nn.softmax(logits)\n",
        "\n",
        "    # Initialize a new session and restore a checkpoint\n",
        "    saver = tf.train.Saver(tf.all_variables())\n",
        "    sess = tf.Session()\n",
        "\n",
        "    saver.restore(sess, FLAGS.test_ckpt_path)\n",
        "    print ('Model restored from ', FLAGS.test_ckpt_path)\n",
        "\n",
        "    prediction_array = np.array([]).reshape(-1, num_classes)\n",
        "    # Test by batches\n",
        "    for step in range(num_batches):\n",
        "      if step % 10 == 0:\n",
        "        print ('%i batches finished!' %step)\n",
        "      offset = step * FLAGS.test_batch_size\n",
        "      test_image_batch = test_image_array[offset:offset+FLAGS.test_batch_size, ...]\n",
        "      batch_prediction_array = sess.run(predictions, feed_dict={self.test_image_placeholder: test_image_batch})\n",
        "      prediction_array = np.concatenate((prediction_array, batch_prediction_array))\n",
        "\n",
        "    # If test_batch_size is not a divisor of num_test_images\n",
        "    if remain_images != 0:\n",
        "      self.test_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[remain_images, img_height, img_width, img_depth])\n",
        "      logits = resnet(self.test_image_placeholder, FLAGS.num_residual_blocks, reuse=True)\n",
        "      predictions = tf.nn.softmax(logits)\n",
        "      test_image_batch = test_image_array[-remain_images:, ...]\n",
        "      batch_prediction_array = sess.run(predictions, feed_dict={self.test_image_placeholder: test_image_batch})\n",
        "      prediction_array = np.concatenate((prediction_array, batch_prediction_array))\n",
        "\n",
        "    return prediction_array\n",
        "\n",
        "# Helper functions \n",
        "\n",
        "  def loss(self, logits, labels):\n",
        "    labels = tf.cast(labels, tf.int64)\n",
        "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=labels)\n",
        "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
        "    return cross_entropy_mean\n",
        "\n",
        "  def top_k_error(self, predictions, labels, k):\n",
        "    batch_size = predictions.get_shape().as_list()[0]\n",
        "    top1_precision = tf.nn.in_top_k(predictions, labels, k=1)\n",
        "    num_correct = tf.reduce_sum(tf.cast(top1_precision, tf.float32))\n",
        "    # self.train_accuracy = (num_correct) / float(batch_size)\n",
        "    return ((batch_size - num_correct) / float(batch_size)) , (num_correct) / float(batch_size)\n",
        "\n",
        "  def train_operation(self, global_step, total_loss, top1_error,accuracy):\n",
        "    tf.summary.scalar('learning_rate', self.lr_ph)\n",
        "    tf.summary.scalar('train_loss', total_loss)\n",
        "    tf.summary.scalar('train_top1_error', top1_error)\n",
        "    tf.summary.scalar('train_accuracy',accuracy)\n",
        "\n",
        "    # To help compute moving avg of training loss and training error \n",
        "    ema = tf.train.ExponentialMovingAverage(FLAGS.train_ema_decay, global_step)\n",
        "    train_ema_op = ema.apply([total_loss, top1_error])\n",
        "    tf.summary.scalar('train_top1_error_avg', ema.average(top1_error))\n",
        "    tf.summary.scalar('train_loss_avg', ema.average(total_loss))\n",
        "\n",
        "    opt = tf.train.MomentumOptimizer(learning_rate=self.lr_ph, momentum=0.9)\n",
        "    train_op = opt.minimize(total_loss, global_step=global_step)\n",
        "    return train_op, train_ema_op\n",
        "\n",
        "  def validation_operation(self, validation_step, top1_error, loss,accuracy):\n",
        "    ema = tf.train.ExponentialMovingAverage(0.0, validation_step) # Decay = 0.0 --> returns original error\n",
        "    ema2 = tf.train.ExponentialMovingAverage(0.95, validation_step)\n",
        "\n",
        "    val_op = tf.group(validation_step.assign_add(1), ema.apply([top1_error, loss]), ema2.apply([top1_error, loss]))\n",
        "    top1_error_val = ema.average(top1_error)\n",
        "    top1_error_avg = ema2.average(top1_error)\n",
        "    loss_val = ema.average(loss)\n",
        "    loss_val_avg = ema2.average(loss)\n",
        "\n",
        "    tf.summary.scalar('val_top1_error', top1_error_val)\n",
        "    tf.summary.scalar('val_top1_error_avg', top1_error_avg)\n",
        "    tf.summary.scalar('val_loss', loss_val)\n",
        "    tf.summary.scalar('val_loss_avg', loss_val_avg)\n",
        "    tf.summary.scalar('val_accuracy',accuracy)\n",
        "    return val_op\n",
        "\n",
        "  def full_validation(self, loss, top1_error, session, vali_data, vali_labels, batch_data,batch_label):\n",
        "    num_batches = 10000 // FLAGS.validation_batch_size\n",
        "    order = np.random.choice(10000, num_batches * FLAGS.validation_batch_size)\n",
        "    vali_data_subset = vali_data[order, ...]\n",
        "    vali_labels_subset = vali_labels[order]\n",
        "\n",
        "    loss_list = []\n",
        "    error_list = []\n",
        "\n",
        "    for step in range(num_batches):\n",
        "      offset = step * FLAGS.validation_batch_size\n",
        "      feed_dict = {self.image_ph: batch_data, self.label_ph: batch_label,\n",
        "        self.valid_image_ph: vali_data_subset[offset:offset+FLAGS.validation_batch_size, ...],\n",
        "        self.valid_label_ph: vali_labels_subset[offset:offset+FLAGS.validation_batch_size],\n",
        "        self.lr_ph: FLAGS.init_lr}\n",
        "      loss_value, top1_error_value = session.run([loss, top1_error], feed_dict=feed_dict)\n",
        "      loss_list.append(loss_value)\n",
        "      error_list.append(top1_error_value)\n",
        "      \n",
        "    return np.mean(loss_list), np.mean(error_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_6QHxpKFjSZ",
        "colab_type": "code",
        "outputId": "731c6ce8-3566-4d8a-d137-f08f2542a203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train = Train_ResNet()\n",
        "train.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shuffling\n",
            "Shuffling\n",
            "WARNING:tensorflow:From <ipython-input-9-1155bc89a28b>:4: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Training\n",
            "-----------------------------------\n",
            "2020-05-05 15:09:57.888828: step 0, loss = 2.3296 (36.0 examples/sec; 3.560 sec/batch)\n",
            "Train top1 err =  0.9140625\n",
            "Valid top1 err = 0.9240\n",
            "Valid loss =  2.3055503\n",
            "-----------------------------------\n",
            "2020-05-05 15:18:11.461123: step 400, loss = 2.0042 (106.8 examples/sec; 1.199 sec/batch)\n",
            "Train top1 err =  0.515625\n",
            "Valid top1 err = 0.5120\n",
            "Valid loss =  1.9726697\n",
            "-----------------------------------\n",
            "2020-05-05 15:26:24.081365: step 800, loss = 1.9956 (103.0 examples/sec; 1.243 sec/batch)\n",
            "Train top1 err =  0.4921875\n",
            "Valid top1 err = 0.4120\n",
            "Valid loss =  1.8941118\n",
            "-----------------------------------\n",
            "2020-05-05 15:34:39.101863: step 1200, loss = 1.8613 (105.7 examples/sec; 1.211 sec/batch)\n",
            "Train top1 err =  0.3671875\n",
            "Valid top1 err = 0.4320\n",
            "Valid loss =  1.8827368\n",
            "-----------------------------------\n",
            "2020-05-05 15:42:48.326619: step 1600, loss = 1.7972 (103.4 examples/sec; 1.237 sec/batch)\n",
            "Train top1 err =  0.2890625\n",
            "Valid top1 err = 0.3840\n",
            "Valid loss =  1.8389874\n",
            "-----------------------------------\n",
            "2020-05-05 15:51:06.848067: step 2000, loss = 1.8271 (105.6 examples/sec; 1.212 sec/batch)\n",
            "Train top1 err =  0.3203125\n",
            "Valid top1 err = 0.3400\n",
            "Valid loss =  1.80336\n",
            "-----------------------------------\n",
            "2020-05-05 15:59:25.902349: step 2400, loss = 1.7635 (105.8 examples/sec; 1.210 sec/batch)\n",
            "Train top1 err =  0.2265625\n",
            "Valid top1 err = 0.3080\n",
            "Valid loss =  1.7768649\n",
            "-----------------------------------\n",
            "2020-05-05 16:07:42.297584: step 2800, loss = 1.7505 (106.7 examples/sec; 1.200 sec/batch)\n",
            "Train top1 err =  0.2265625\n",
            "Valid top1 err = 0.3120\n",
            "Valid loss =  1.7609683\n",
            "-----------------------------------\n",
            "2020-05-05 16:15:55.491795: step 3200, loss = 1.7934 (105.1 examples/sec; 1.218 sec/batch)\n",
            "Train top1 err =  0.296875\n",
            "Valid top1 err = 0.2600\n",
            "Valid loss =  1.7382485\n",
            "-----------------------------------\n",
            "2020-05-05 16:24:08.264799: step 3600, loss = 1.7317 (105.4 examples/sec; 1.214 sec/batch)\n",
            "Train top1 err =  0.203125\n",
            "Valid top1 err = 0.2120\n",
            "Valid loss =  1.6858897\n",
            "-----------------------------------\n",
            "2020-05-05 16:32:22.782661: step 4000, loss = 1.7326 (98.7 examples/sec; 1.297 sec/batch)\n",
            "Train top1 err =  0.2265625\n",
            "Valid top1 err = 0.2600\n",
            "Valid loss =  1.7161927\n",
            "-----------------------------------\n",
            "2020-05-05 16:40:41.299799: step 4400, loss = 1.8058 (104.6 examples/sec; 1.223 sec/batch)\n",
            "Train top1 err =  0.28125\n",
            "Valid top1 err = 0.2320\n",
            "Valid loss =  1.6934409\n",
            "-----------------------------------\n",
            "2020-05-05 16:49:02.066927: step 4800, loss = 1.7254 (104.2 examples/sec; 1.228 sec/batch)\n",
            "Train top1 err =  0.1953125\n",
            "Valid top1 err = 0.2760\n",
            "Valid loss =  1.7359413\n",
            "-----------------------------------\n",
            "2020-05-05 16:57:17.026497: step 5200, loss = 1.6907 (102.8 examples/sec; 1.245 sec/batch)\n",
            "Train top1 err =  0.1640625\n",
            "Valid top1 err = 0.2080\n",
            "Valid loss =  1.6734481\n",
            "-----------------------------------\n",
            "2020-05-05 17:05:26.911987: step 5600, loss = 1.6913 (107.8 examples/sec; 1.187 sec/batch)\n",
            "Train top1 err =  0.1875\n",
            "Valid top1 err = 0.2520\n",
            "Valid loss =  1.7125019\n",
            "-----------------------------------\n",
            "2020-05-05 17:13:28.116224: step 6000, loss = 1.7098 (106.8 examples/sec; 1.199 sec/batch)\n",
            "Train top1 err =  0.1875\n",
            "Valid top1 err = 0.2440\n",
            "Valid loss =  1.6908209\n",
            "-----------------------------------\n",
            "2020-05-05 17:21:40.528037: step 6400, loss = 1.6758 (108.2 examples/sec; 1.183 sec/batch)\n",
            "Train top1 err =  0.15625\n",
            "Valid top1 err = 0.2360\n",
            "Valid loss =  1.6911142\n",
            "-----------------------------------\n",
            "2020-05-05 17:29:52.137393: step 6800, loss = 1.6403 (107.9 examples/sec; 1.186 sec/batch)\n",
            "Train top1 err =  0.1171875\n",
            "Valid top1 err = 0.2040\n",
            "Valid loss =  1.6675876\n",
            "-----------------------------------\n",
            "2020-05-05 17:37:58.010099: step 7200, loss = 1.7333 (109.2 examples/sec; 1.173 sec/batch)\n",
            "Train top1 err =  0.2109375\n",
            "Valid top1 err = 0.2240\n",
            "Valid loss =  1.6825174\n",
            "-----------------------------------\n",
            "2020-05-05 17:46:05.725584: step 7600, loss = 1.7117 (106.7 examples/sec; 1.199 sec/batch)\n",
            "Train top1 err =  0.171875\n",
            "Valid top1 err = 0.2120\n",
            "Valid loss =  1.6741892\n",
            "-----------------------------------\n",
            "2020-05-05 17:54:10.878979: step 8000, loss = 1.7014 (107.8 examples/sec; 1.187 sec/batch)\n",
            "Train top1 err =  0.1640625\n",
            "Valid top1 err = 0.1480\n",
            "Valid loss =  1.618258\n",
            "-----------------------------------\n",
            "2020-05-05 18:02:20.631142: step 8400, loss = 1.7261 (106.5 examples/sec; 1.202 sec/batch)\n",
            "Train top1 err =  0.2109375\n",
            "Valid top1 err = 0.1880\n",
            "Valid loss =  1.650579\n",
            "-----------------------------------\n",
            "2020-05-05 18:10:26.932978: step 8800, loss = 1.6688 (103.2 examples/sec; 1.240 sec/batch)\n",
            "Train top1 err =  0.140625\n",
            "Valid top1 err = 0.2160\n",
            "Valid loss =  1.6709703\n",
            "-----------------------------------\n",
            "2020-05-05 18:18:30.519806: step 9200, loss = 1.6848 (107.4 examples/sec; 1.192 sec/batch)\n",
            "Train top1 err =  0.1640625\n",
            "Valid top1 err = 0.1720\n",
            "Valid loss =  1.6452311\n",
            "-----------------------------------\n",
            "2020-05-05 18:26:38.063906: step 9600, loss = 1.6321 (107.3 examples/sec; 1.193 sec/batch)\n",
            "Train top1 err =  0.1015625\n",
            "Valid top1 err = 0.2200\n",
            "Valid loss =  1.6738843\n",
            "-----------------------------------\n",
            "2020-05-05 18:34:39.578140: step 10000, loss = 1.7066 (107.9 examples/sec; 1.187 sec/batch)\n",
            "Train top1 err =  0.1875\n",
            "Valid top1 err = 0.1600\n",
            "Valid loss =  1.633683\n",
            "-----------------------------------\n",
            "2020-05-05 18:42:41.301364: step 10400, loss = 1.6641 (108.5 examples/sec; 1.179 sec/batch)\n",
            "Train top1 err =  0.1328125\n",
            "Valid top1 err = 0.1880\n",
            "Valid loss =  1.6480877\n",
            "-----------------------------------\n",
            "2020-05-05 18:50:49.735862: step 10800, loss = 1.6938 (105.1 examples/sec; 1.218 sec/batch)\n",
            "Train top1 err =  0.1640625\n",
            "Valid top1 err = 0.1760\n",
            "Valid loss =  1.6435153\n",
            "-----------------------------------\n",
            "Learning rate after decay  0.010000000000000002\n",
            "2020-05-05 18:59:06.635762: step 11200, loss = 1.6445 (106.3 examples/sec; 1.204 sec/batch)\n",
            "Train top1 err =  0.109375\n",
            "Valid top1 err = 0.1520\n",
            "Valid loss =  1.610043\n",
            "-----------------------------------\n",
            "2020-05-05 19:07:22.824616: step 11600, loss = 1.5811 (106.3 examples/sec; 1.205 sec/batch)\n",
            "Train top1 err =  0.0546875\n",
            "Valid top1 err = 0.1640\n",
            "Valid loss =  1.6210333\n",
            "-----------------------------------\n",
            "2020-05-05 19:15:29.002613: step 12000, loss = 1.6293 (109.2 examples/sec; 1.172 sec/batch)\n",
            "Train top1 err =  0.09375\n",
            "Valid top1 err = 0.1240\n",
            "Valid loss =  1.584685\n",
            "-----------------------------------\n",
            "2020-05-05 19:23:29.442671: step 12400, loss = 1.5860 (108.6 examples/sec; 1.178 sec/batch)\n",
            "Train top1 err =  0.078125\n",
            "Valid top1 err = 0.1720\n",
            "Valid loss =  1.636107\n",
            "-----------------------------------\n",
            "2020-05-05 19:31:33.963077: step 12800, loss = 1.6313 (106.7 examples/sec; 1.199 sec/batch)\n",
            "Train top1 err =  0.1171875\n",
            "Valid top1 err = 0.1160\n",
            "Valid loss =  1.5919393\n",
            "-----------------------------------\n",
            "2020-05-05 19:39:40.741542: step 13200, loss = 1.6036 (107.1 examples/sec; 1.195 sec/batch)\n",
            "Train top1 err =  0.0859375\n",
            "Valid top1 err = 0.1560\n",
            "Valid loss =  1.6255741\n",
            "-----------------------------------\n",
            "2020-05-05 19:47:48.996254: step 13600, loss = 1.6408 (107.5 examples/sec; 1.191 sec/batch)\n",
            "Train top1 err =  0.1171875\n",
            "Valid top1 err = 0.1600\n",
            "Valid loss =  1.6171213\n",
            "-----------------------------------\n",
            "2020-05-05 19:55:51.065089: step 14000, loss = 1.6384 (107.2 examples/sec; 1.194 sec/batch)\n",
            "Train top1 err =  0.1171875\n",
            "Valid top1 err = 0.1280\n",
            "Valid loss =  1.6000324\n",
            "-----------------------------------\n",
            "2020-05-05 20:04:00.623117: step 14400, loss = 1.5992 (103.9 examples/sec; 1.231 sec/batch)\n",
            "Train top1 err =  0.0703125\n",
            "Valid top1 err = 0.1400\n",
            "Valid loss =  1.607111\n",
            "-----------------------------------\n",
            "2020-05-05 20:12:04.397734: step 14800, loss = 1.5998 (108.6 examples/sec; 1.179 sec/batch)\n",
            "Train top1 err =  0.0703125\n",
            "Valid top1 err = 0.1440\n",
            "Valid loss =  1.6053659\n",
            "-----------------------------------\n",
            "2020-05-05 20:20:04.303074: step 15200, loss = 1.6010 (109.6 examples/sec; 1.168 sec/batch)\n",
            "Train top1 err =  0.0625\n",
            "Valid top1 err = 0.1600\n",
            "Valid loss =  1.6276404\n",
            "-----------------------------------\n",
            "2020-05-05 20:28:08.630580: step 15600, loss = 1.5959 (107.9 examples/sec; 1.186 sec/batch)\n",
            "Train top1 err =  0.078125\n",
            "Valid top1 err = 0.1680\n",
            "Valid loss =  1.6471522\n",
            "-----------------------------------\n",
            "2020-05-05 20:36:14.325708: step 16000, loss = 1.6486 (106.1 examples/sec; 1.207 sec/batch)\n",
            "Train top1 err =  0.1171875\n",
            "Valid top1 err = 0.1560\n",
            "Valid loss =  1.6121454\n",
            "-----------------------------------\n",
            "Learning rate after decay  0.0010000000000000002\n",
            "2020-05-05 20:44:32.291782: step 16400, loss = 1.6140 (105.6 examples/sec; 1.213 sec/batch)\n",
            "Train top1 err =  0.0859375\n",
            "Valid top1 err = 0.1280\n",
            "Valid loss =  1.600196\n",
            "-----------------------------------\n",
            "2020-05-05 20:52:44.122197: step 16800, loss = 1.6000 (108.4 examples/sec; 1.181 sec/batch)\n",
            "Train top1 err =  0.078125\n",
            "Valid top1 err = 0.1680\n",
            "Valid loss =  1.6259634\n",
            "-----------------------------------\n",
            "2020-05-05 21:00:58.152188: step 17200, loss = 1.6210 (104.2 examples/sec; 1.228 sec/batch)\n",
            "Train top1 err =  0.1015625\n",
            "Valid top1 err = 0.1600\n",
            "Valid loss =  1.6222363\n",
            "-----------------------------------\n",
            "2020-05-05 21:09:04.279378: step 17600, loss = 1.6354 (108.7 examples/sec; 1.177 sec/batch)\n",
            "Train top1 err =  0.109375\n",
            "Valid top1 err = 0.1600\n",
            "Valid loss =  1.6180197\n",
            "-----------------------------------\n",
            "2020-05-05 21:17:01.984405: step 18000, loss = 1.6495 (110.9 examples/sec; 1.154 sec/batch)\n",
            "Train top1 err =  0.1171875\n",
            "Valid top1 err = 0.1400\n",
            "Valid loss =  1.6139398\n",
            "-----------------------------------\n",
            "2020-05-05 21:25:04.392485: step 18400, loss = 1.5992 (104.8 examples/sec; 1.222 sec/batch)\n",
            "Train top1 err =  0.0703125\n",
            "Valid top1 err = 0.1480\n",
            "Valid loss =  1.6177385\n",
            "-----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujr1QHiGPEC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8cRGEs5JMio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nXAlQiFFc2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs_test_3"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}