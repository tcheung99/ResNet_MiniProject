{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3jFbdoWJYPdYcGb6QGbmV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcheung99/ResNet_MiniProject/blob/master/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KaFIpmHWgDV",
        "colab_type": "text"
      },
      "source": [
        "**ResNet in Tensorflow for CIFAR-10** \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The following formulation is based on He et al.'s [paper](https://arxiv.org/abs/1512.03385).\n",
        "\n",
        "General design decisions:\n",
        "\n",
        "*   Full pre-activation residual block (see: [residual block variants](https://miro.medium.com/max/1400/1*M5NIelQC33eN6KjwZRccoQ.png))\n",
        "\n",
        "*   Perform 1x1 convolution insteaad of padding input (projection shortcut) for matching output size \n",
        "\n",
        "Formulation based on [paper](https://arxiv.org/abs/1512.03385): \n",
        "*   1st layer is 3x3 convolutions \n",
        "*   Stack of 6n layers with 3x3 convolutions on feature maps of size {32,16,8} respectively (n = number of residual blocks) \n",
        "*   Ends with global average pooling, a FC layer, and softmax\n",
        "\n",
        "About [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz):\n",
        "\n",
        "*   Image size = 32x32\n",
        "*   Label Dimension = 10 (classes)\n",
        "*   60000 images total - 50000 training, 10000 testing \n",
        "*   5 training batches, 1 test batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un2eonVMoHoH",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlusWoelduH8",
        "colab_type": "code",
        "outputId": "d62a84ea-38b9-4670-b091-d5b0e7dae1f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf \n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tyuiupzld805",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCDJrBtHj5-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "from six.moves import urllib\n",
        "import sys\n",
        "import numpy as np\n",
        "import _pickle as cPickle\n",
        "import os\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2_ayBRhQ_D0",
        "colab_type": "text"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEGJWJAN0-yA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_width = 32\n",
        "img_height = 32\n",
        "img_depth = 3\n",
        "\n",
        "num_train_batches = 5 \n",
        "num_epochs = 10000 * num_train_batches\n",
        "num_classes = 10\n",
        "\n",
        "data_dir = 'cifar10_data'\n",
        "full_data_dir = data_dir + '/cifar-10-batches-py/data_batch_'\n",
        "valid_dir = data_dir + '/cifar-10-batches-py/test_batch'\n",
        "URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "filename = 'cifar-10-python.tar.gz'\n",
        "filepath = os.path.join(data_dir, filename)\n",
        "\n",
        "if not os.path.exists(filepath):\n",
        "    def _progress(count, block_size, total_size):\n",
        "        sys.stdout.write('\\r>> Downloading... %s %.1f%%' % (filename, float(count * block_size)/ float(total_size) * 100.0))\n",
        "        sys.stdout.flush()\n",
        "    filepath, _ = urllib.request.urlretrieve(URL, filepath, _progress)\n",
        "    print()\n",
        "    statinfo = os.stat(filepath)\n",
        "    print('Success! Downloaded:', filename, statinfo.st_size, 'bytes.')\n",
        "    tarfile.open(filepath, 'r:gz').extractall(data_dir)\n",
        "\n",
        "def read_all(all_paths, shuffle=True, use_random_label = False):\n",
        "    data = np.array([]).reshape([0, img_width * img_height * img_depth])\n",
        "    label = np.array([])\n",
        "\n",
        "    for path in all_paths:\n",
        "        batch_data, batch_label = read_one_batch(path, use_random_label)\n",
        "        # np.concatenate concatenates along the 0-axis\n",
        "        data = np.concatenate((data, batch_data))\n",
        "        label = np.concatenate((label, batch_label))\n",
        "    print('Read all data')\n",
        "    num = len(label)\n",
        "    data = data.reshape((num, img_height * img_width, img_depth), order='F')\n",
        "    data = data.reshape((num, img_height, img_width, img_depth))\n",
        "\n",
        "    if shuffle:\n",
        "        print('Shuffle')\n",
        "        # Random shuffle\n",
        "        order = np.random.permutation(num)\n",
        "        data = data[order, ...]\n",
        "        label = label[order]\n",
        "\n",
        "    data = data.astype(np.float32)\n",
        "    return data, label\n",
        "\n",
        "def read_one_batch(path, use_random_label):\n",
        "    infile = open(path, 'rb')\n",
        "    new_dict = cPickle.load(infile, encoding='latin1')\n",
        "    infile.close()\n",
        "\n",
        "    image = new_dict['data']\n",
        "    if use_random_label: # Adding random labels \n",
        "        labels = np.random.randint(low=0, high=10, size=10000)\n",
        "        label = np.array(labels)\n",
        "    else:\n",
        "        label = np.array(new_dict['labels'])\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "def crop_and_flip(batch_images, padding_size):\n",
        "    cropped_batch = np.zeros(len(batch_images)*img_depth*img_height*img_width).reshape(len(batch_images), img_height, img_width, img_depth)\n",
        "    flip_prob = np.random.randint(low=0, high=2)\n",
        "\n",
        "    # Random cropping \n",
        "    for i in range(len(batch_images)):\n",
        "        x_offset = np.random.randint(low=0, high=2*padding_size, size=1)[0]\n",
        "        y_offset = np.random.randint(low=0, high=2*padding_size, size=1)[0]\n",
        "        cropped_batch[i, ...] = batch_images[i, ...][x_offset:(x_offset+img_height),y_offset:(y_offset+img_width), :]\n",
        "\n",
        "    # Random horizontal flipping\n",
        "    if flip_prob == 0:\n",
        "        cropped_batch[i, ...] = cv2.flip(cropped_batch[i, ...], 1) # Flip along axis 1 \n",
        "    return cropped_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YT0wRDzu5_c",
        "colab_type": "text"
      },
      "source": [
        "# Defining Hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCm_jg9S38D2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS = tf.app.flags.FLAGS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCdO1kyGSTaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DEFINE args: flagname, defaultvalue, docstring\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "# Checkpoint paths \n",
        "tf.app.flags.DEFINE_string('checkpoint_path', 'cache/logs_repeat20/model.ckpt-20000', 'Checkpoint directory')\n",
        "tf.app.flags.DEFINE_boolean('use_checkpoint', False, 'Loading checkpoint?')\n",
        "tf.app.flags.DEFINE_string('test_checkpoint_path', 'model_1.ckpt-19999', 'Checkpoint directory to restore')\n",
        "tf.app.flags.DEFINE_string('version', 'test_6', 'Version number for logs and checkpoints')\n",
        "\n",
        "# Input handling/augmentation \n",
        "tf.app.flags.DEFINE_integer('padding_size', 2, 'Number of zero padding layers on the sides of input data/image')\n",
        "\n",
        "# Reporting and tensorboard\n",
        "tf.app.flags.DEFINE_integer('report_frequency', 400, '''Steps takes to output errors on the screen and write summaries''')\n",
        "tf.app.flags.DEFINE_float('train_ema_decay', 0.95, 'Decay factor for training error`s exponential moving average')\n",
        "\n",
        "# Batch size \n",
        "tf.app.flags.DEFINE_integer('train_batch_size', 128, 'Train batch size')\n",
        "tf.app.flags.DEFINE_integer('validation_batch_size', 250, 'Validation batch size')\n",
        "tf.app.flags.DEFINE_integer('test_batch_size', 125, 'Test batch size')\n",
        "\n",
        "# Training parameters \n",
        "tf.app.flags.DEFINE_boolean('is_full_validation', False, 'Validate using the entire validation set or random batch')\n",
        "\n",
        "tf.app.flags.DEFINE_integer('num_residual_blocks', 2, 'Number of residual blocks')\n",
        "\n",
        "tf.app.flags.DEFINE_integer('train_steps', 20000, 'Number of steps for training')\n",
        "tf.app.flags.DEFINE_float('weight_decay', 0.0001, 'Scale for l2 regularization')\n",
        "\n",
        "tf.app.flags.DEFINE_float('init_lr', 0.1, 'Initial learning rate')\n",
        "tf.app.flags.DEFINE_float('lr_decay_factor', 0.1, 'Factor to decay the learning rate')\n",
        "tf.app.flags.DEFINE_integer('decay_step0', 11000, 'First step to decay the learning rate')\n",
        "tf.app.flags.DEFINE_integer('decay_step1', 16000, 'Second step to decay the learning rate')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiXXcg2TzgMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory for training logs, checkpoints, and errors\n",
        "train_dir = 'logs_' + FLAGS.version + '/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfgPeq6XRFqZ",
        "colab_type": "text"
      },
      "source": [
        "# Defining ResNet functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_idz0EPXudU",
        "colab_type": "text"
      },
      "source": [
        "A residual block is when activation of a layer is \"fast-forwarded\" to a deeper layer \n",
        "\n",
        "*   Passing input through BN, relu, conv : conv(relu(batch_norm(x)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-i5ybxpWkR9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "48e5b6a9-6ed5-4158-b885-0d6c64e1aef3"
      },
      "source": [
        "def make_var(name,shape,initializer=tf.contrib.layers.xavier_initializer(),is_fc_layer=False):\n",
        "  # initializer = tf.contrib.layers.xavier_initializer()\n",
        "  regularizer = tf.contrib.layers.l2_regularizer(scale=FLAGS.weight_decay)\n",
        "  new_var = tf.get_variable(name, shape=shape, initializer=initializer, regularizer=regularizer)\n",
        "  return new_var"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtHby_0fwuFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bn_layer(input_layer, input_depth):\n",
        "  # Batch norm layer \n",
        "  mean, var = tf.nn.moments(input_layer, axes=[0,1,2])\n",
        "  beta = tf.get_variable('beta', input_depth, tf.float32, tf.constant_initializer(0.0,tf.float32))\n",
        "  gamma = tf.get_variable('gamma', input_depth, tf.float32, tf.constant_initializer(1.0,tf.float32))\n",
        "  BN_layer = tf.nn.batch_norm_with_global_normalization(input_layer, mean, var, beta,gamma, 0.001)\n",
        "  return BN_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMl1S0Khm1f5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_layer(input_layer, input_depth, filter_shape,stride):\n",
        "  BN_layer = bn_layer(input_layer, input_depth)\n",
        "\n",
        "  # Apply ReLu \n",
        "  ReLu_layer = tf.nn.relu(BN_layer)\n",
        "\n",
        "  # Conv layer \n",
        "  filter = make_var(name='conv',shape=filter_shape)\n",
        "  conv_layer = tf.nn.conv2d(ReLu_layer, filter=filter,strides=[1,stride,stride,1],padding='SAME')\n",
        "\n",
        "  return conv_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geiMvRkzqgth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def res_block(input_layer, output_depth, first_block=False):\n",
        "  input_depth = input_layer.get_shape().as_list()[-1]\n",
        "  if input_depth == output_depth:\n",
        "    project = False \n",
        "    stride = 1 \n",
        "  elif input_depth != output_depth:\n",
        "    project = True \n",
        "    stride = 2 \n",
        "  with tf.variable_scope('conv1_in'):\n",
        "    if first_block:\n",
        "      filter = make_var('conv',[3,3,input_depth,output_depth])    \n",
        "      conv1 = tf.nn.conv2d(input_layer,filter=filter,strides=[1,1,1,1],padding='SAME')\n",
        "    elif not first_block:\n",
        "      conv1 = conv_layer(input_layer,input_depth, [3,3,input_depth,output_depth],stride)\n",
        "\n",
        "  with tf.variable_scope('conv2_in'):\n",
        "    conv2 = conv_layer(conv1, conv1.get_shape().as_list()[-1], [3,3,output_depth,output_depth],1)\n",
        "\n",
        "  if project == True:\n",
        "    # 1x1 conv shortcut projection \n",
        "    # mod_input = tf.nn.conv2d(input_layer, [1,1,input_depth,output_depth], stride)\n",
        "\n",
        "    mod_input = conv_layer(input_layer, input_depth, [1,1,input_depth,output_depth],2)\n",
        "\n",
        "    # pooled_input = tf.nn.avg_pool(input_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "    # padded_input = tf.pad(pooled_input, [[0, 0], [0, 0], [0, 0], [input_depth // 2,input_depth // 2]])                                                          \n",
        "    # mod_input = padded_input\n",
        "  elif project == False:\n",
        "    mod_input = input_layer\n",
        "\n",
        "  result = conv2 + mod_input\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTzF-F2hxB0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_layer(input_layer, dimensions):\n",
        "  input_depth = input_layer.get_shape().as_list()[-1]\n",
        "  BN_layer = bn_layer(input_layer, input_depth)\n",
        "  ReLu_layer = tf.nn.relu(BN_layer)\n",
        "  pool_layer = tf.reduce_mean(ReLu_layer, [1, 2])\n",
        "\n",
        "  assert pool_layer.get_shape().as_list()[-1:] == [64]\n",
        "  ##\n",
        "  fc_w = make_var(name='fc_weights', shape=[input_depth, dimensions],initializer=tf.uniform_unit_scaling_initializer(factor=1.0), is_fc_layer=True)\n",
        "  fc_b = make_var(name='fc_bias', shape=[dimensions], initializer=tf.zeros_initializer())\n",
        "  fc_h = tf.nn.softmax(tf.matmul(pool_layer, fc_w) + fc_b)\n",
        "  \n",
        "  return fc_h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QAy4l5ixNTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_summary(t):\n",
        "  name = t.op.name\n",
        "  tf.summary.histogram(name+'/activations', t)\n",
        "  tf.summary.scalar(name + '/sparsity', tf.nn.zero_fraction(t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1HMtdiCumvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet(input_tensor,n,reuse):\n",
        "\n",
        "  all_layers = list()\n",
        "\n",
        "  # Op on input tensor\n",
        "  with tf.variable_scope('conv0',reuse=reuse):\n",
        "    filter = make_var(name='conv',shape=[3,3,3,16])\n",
        "    conv_layer = tf.nn.conv2d(input_tensor, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
        "    BN_layer = bn_layer(conv_layer, 16)\n",
        "    conv0 = tf.nn.relu(BN_layer)\n",
        "    tensor_summary(conv0)\n",
        "    all_layers.append(conv0)\n",
        "\n",
        "  for i in range(n):\n",
        "    with tf.variable_scope('conv1_%d' %i, reuse=reuse):\n",
        "      if i == 0:\n",
        "          conv1 = res_block(all_layers[-1], 16, first_block=True)\n",
        "      else:\n",
        "          conv1 = res_block(all_layers[-1], 16)\n",
        "      tensor_summary(conv1)\n",
        "      all_layers.append(conv1)\n",
        "\n",
        "  for i in range(n):\n",
        "    with tf.variable_scope('conv2_%d' %i, reuse=reuse):\n",
        "      conv2 = res_block(all_layers[-1], 32)\n",
        "      tensor_summary(conv2)\n",
        "      all_layers.append(conv2)\n",
        "\n",
        "  for i in range(n):\n",
        "    with tf.variable_scope('conv3_%d' %i, reuse=reuse):\n",
        "      conv3 = res_block(all_layers[-1], 64)\n",
        "      tensor_summary(conv3)\n",
        "      all_layers.append(conv3)\n",
        "    assert conv3.get_shape().as_list()[1:] == [8, 8, 64]\n",
        "\n",
        "  with tf.variable_scope('fc', reuse=reuse):\n",
        "    fc = fc_layer(all_layers[-1],10)\n",
        "    all_layers.append(fc)\n",
        "\n",
        "  return all_layers[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GakRQ8w0A2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_graph(train_dir='logs'):\n",
        "  input_tensor = tf.constant(np.ones([128, 32, 32, 3]), dtype=tf.float32)\n",
        "  output = resnet(input_tensor, reuse=False, n=2)\n",
        "  init = tf.initialize_all_variables()\n",
        "  session = tf.Session()\n",
        "  session.run(init)\n",
        "  summary_writer = tf.train.SummaryWriter(train_dir, sess.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rdCC6ZlSC7j",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5gv0kHx1A-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Train_ResNet(object):\n",
        "  def __init__(self):\n",
        "    # Initialize placeholders\n",
        "    self.lr_ph = tf.placeholder(dtype=tf.float32, shape=[])\n",
        "    self.image_ph = tf.placeholder(dtype=tf.float32, shape=[FLAGS.train_batch_size, img_height,img_width, img_depth])\n",
        "    self.label_ph = tf.placeholder(dtype=tf.int32, shape=[FLAGS.train_batch_size])\n",
        "    self.valid_image_ph = tf.placeholder(dtype=tf.float32, shape=[FLAGS.validation_batch_size, img_height, img_width, img_depth])\n",
        "    self.valid_label_ph = tf.placeholder(dtype=tf.int32, shape=[FLAGS.validation_batch_size])\n",
        "\n",
        "  def build_graph(self):\n",
        "    \n",
        "    # Training\n",
        "    logits = resnet(self.image_ph, FLAGS.num_residual_blocks, reuse=False)\n",
        "    # Validation: Reuse weight from training, reuse=True \n",
        "    valid_logits = resnet(self.valid_image_ph, FLAGS.num_residual_blocks, reuse=True)\n",
        "\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "    validation_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # Training loss \n",
        "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    loss = self.cross_entropy_loss(logits, self.label_ph)\n",
        "    self.all_loss = tf.add_n([loss] + reg_losses)\n",
        "\n",
        "    predictions = tf.nn.softmax(logits)\n",
        "    self.train_top1_error, self.train_acc = self.error_acc(predictions, self.label_ph, 1)\n",
        "\n",
        "    # Validation loss\n",
        "    self.valid_loss = self.cross_entropy_loss(valid_logits, self.valid_label_ph)\n",
        "    vali_predictions = tf.nn.softmax(valid_logits)\n",
        "    self.valid_top1_error, self.valid_acc = self.error_acc(vali_predictions, self.valid_label_ph, 1)\n",
        "\n",
        "    self.train_op, self.train_ema_op = self.train_op(global_step, self.all_loss,self.train_top1_error,self.train_acc)\n",
        "    self.val_op = self.validation_op(validation_step, self.valid_top1_error, self.valid_loss,self.valid_acc)\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    train_paths = list() \n",
        "    for i in range(1, num_train_batches+1):\n",
        "        train_paths.append(full_data_dir + str(i))\n",
        "    all_data, all_labels = read_all(train_paths, use_random_label=False)     \n",
        "    \n",
        "    pad_width = ((0, 0), (FLAGS.padding_size, FLAGS.padding_size), (FLAGS.padding_size, FLAGS.padding_size), (0, 0))\n",
        "    all_data = np.pad(all_data, pad_width=pad_width, mode='constant', constant_values=0)\n",
        "\n",
        "    valid_data, valid_labels = read_all([valid_dir],use_random_label=False)\n",
        "\n",
        "    # Build train and validation graph\n",
        "    self.build_graph()\n",
        "\n",
        "    # Saver to save checkpoints \n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    summary_op = tf.summary.merge_all()\n",
        "    init = tf.initialize_all_variables()\n",
        "    sess = tf.Session()\n",
        "\n",
        "    # Loading from a checkpoint\n",
        "    if FLAGS.use_checkpoint is True:\n",
        "      saver.restore(sess, FLAGS.checkpoint_path)\n",
        "      print ('Restored from checkpoint')\n",
        "    else:\n",
        "      sess.run(init)\n",
        "\n",
        "    summary_writer = tf.summary.FileWriter(train_dir, sess.graph)\n",
        "\n",
        "    step_list = []\n",
        "    train_error_list = []\n",
        "    val_error_list = []\n",
        "\n",
        "    for step in range(FLAGS.train_steps):\n",
        "      train_offset = np.random.choice(num_epochs - FLAGS.train_batch_size, 1)[0]\n",
        "      batch_data = all_data[train_offset:train_offset + FLAGS.train_batch_size, ...]\n",
        "      train_batch_data = crop_and_flip(batch_data, padding_size=FLAGS.padding_size)\n",
        "      train_batch_labels = all_labels[train_offset:train_offset+FLAGS.train_batch_size]\n",
        "\n",
        "      val_offset = np.random.choice(10000 - FLAGS.validation_batch_size, 1)[0]\n",
        "      validation_batch_data = valid_data[val_offset:val_offset+FLAGS.validation_batch_size, ...]\n",
        "      validation_batch_labels = valid_labels[val_offset:val_offset+FLAGS.validation_batch_size]\n",
        "\n",
        "      # Want to validate once before training\n",
        "      if step % FLAGS.report_frequency == 0:\n",
        "        if FLAGS.is_full_validation is True:\n",
        "          validation_loss_value, validation_error_value = self.full_validation(\n",
        "                                        loss=self.valid_loss,\n",
        "                                        top1_error=self.valid_top1_error, vali_data=valid_data,\n",
        "                                        vali_labels=valid_labels, session=sess,\n",
        "                                        batch_data=train_batch_data, batch_label=train_batch_labels)\n",
        "          vali_summ = tf.Summary()\n",
        "          vali_summ.value.add(tag='full_validation_error',\n",
        "                              simple_value=validation_error_value.astype(np.float))\n",
        "          summary_writer.add_summary(vali_summ, step)\n",
        "          summary_writer.flush()\n",
        "\n",
        "        else:\n",
        "          # Run TF Session for Validation (use validation optimizer)\n",
        "          _, validation_error_value, validation_loss_value = sess.run(\n",
        "                                        [self.val_op,self.valid_top1_error,self.valid_loss],\n",
        "                                        {self.image_ph: train_batch_data,\n",
        "                                          self.label_ph: train_batch_labels,\n",
        "                                          self.valid_image_ph: validation_batch_data,\n",
        "                                          self.valid_label_ph: validation_batch_labels,\n",
        "                                          self.lr_ph: FLAGS.init_lr})\n",
        "\n",
        "          val_error_list.append(validation_error_value)\n",
        "\n",
        "      start_time = time.time()\n",
        "      _, _, train_loss_value, train_error_value = sess.run(\n",
        "                                          [self.train_op, self.train_ema_op,self.all_loss, self.train_top1_error],\n",
        "                                          {self.image_ph: train_batch_data,\n",
        "                                          self.label_ph: train_batch_labels,\n",
        "                                          self.valid_image_ph: validation_batch_data,\n",
        "                                          self.valid_label_ph: validation_batch_labels,\n",
        "                                          self.lr_ph: FLAGS.init_lr})\n",
        "      duration = time.time() - start_time\n",
        "\n",
        "      if step % FLAGS.report_frequency == 0:\n",
        "        summary_str = sess.run(summary_op, {self.image_ph: train_batch_data,\n",
        "                                            self.label_ph: train_batch_labels,\n",
        "                                            self.valid_image_ph: validation_batch_data,\n",
        "                                            self.valid_label_ph: validation_batch_labels,\n",
        "                                            self.lr_ph: FLAGS.init_lr})\n",
        "        summary_writer.add_summary(summary_str, step)\n",
        "\n",
        "        num_examples_per_step = FLAGS.train_batch_size\n",
        "        examples_per_sec = num_examples_per_step / duration\n",
        "        sec_per_batch = float(duration)\n",
        "\n",
        "        # Report err and loss at report freq \n",
        "\n",
        "        # format_str = ('%s: step %d, loss = %.4f (%.1f examples/sec; %.3f ' 'sec/batch)')\n",
        "        print (('%s: step %d, loss = %.4f (%.1f examples/sec; %.3f ' 'sec/batch)') % (datetime.now(), step, train_loss_value, examples_per_sec,sec_per_batch))\n",
        "        print ('Train top1 err = ', train_error_value)\n",
        "        print ('Valid top1 err = %.4f' % validation_error_value)\n",
        "        print ('Valid loss = ', validation_loss_value)\n",
        "\n",
        "        step_list.append(step)\n",
        "        train_error_list.append(train_error_value)\n",
        "\n",
        "      # Decay learning rate \n",
        "      if step == FLAGS.decay_step0 or step == FLAGS.decay_step1:\n",
        "        FLAGS.init_lr = 0.1 * FLAGS.init_lr\n",
        "        print ('Learning rate after decay', FLAGS.init_lr)\n",
        "\n",
        "      # Save checkpoint every 5000 steps\n",
        "      if step % 5000 == 0 or (step + 1) == FLAGS.train_steps:\n",
        "        checkpoint_path = os.path.join(train_dir, 'model.ckpt')\n",
        "        saver.save(sess, checkpoint_path, global_step=step)\n",
        "\n",
        "        # Write to csv \n",
        "        df = pd.DataFrame(data={'step':step_list, 'train_error':train_error_list,'validation_error': val_error_list})\n",
        "        df.to_csv(train_dir + FLAGS.version + '_error.csv')\n",
        "\n",
        "\n",
        "  def test(self, test_image_array):\n",
        "    num_test_images = len(test_image_array)\n",
        "    num_batches = num_test_images // FLAGS.test_batch_size\n",
        "    remain_images = num_test_images % FLAGS.test_batch_size\n",
        "    print ('%i test batches in total...' %num_batches)\n",
        "\n",
        "    # Create the test image and labels placeholders\n",
        "    self.test_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[FLAGS.test_batch_size, img_height, img_width, img_depth])\n",
        "\n",
        "    # Build the test graph\n",
        "    logits = resnet(self.test_image_placeholder, FLAGS.num_residual_blocks, reuse=False)\n",
        "    predictions = tf.nn.softmax(logits)\n",
        "\n",
        "    # Initialize a new session and restore a checkpoint\n",
        "    saver = tf.train.Saver(tf.all_variables())\n",
        "    sess = tf.Session()\n",
        "\n",
        "    saver.restore(sess, FLAGS.test_checkpoint_path)\n",
        "    print ('Model restored from ', FLAGS.test_checkpoint_path)\n",
        "\n",
        "    prediction_array = np.array([]).reshape(-1, num_classes)\n",
        "    # Test by batches\n",
        "    for step in range(num_batches):\n",
        "      if step % 10 == 0:\n",
        "        print ('%i batches finished!' %step)\n",
        "      offset = step * FLAGS.test_batch_size\n",
        "      test_image_batch = test_image_array[offset:offset+FLAGS.test_batch_size, ...]\n",
        "      batch_prediction_array = sess.run(predictions, feed_dict={self.test_image_placeholder: test_image_batch})\n",
        "      prediction_array = np.concatenate((prediction_array, batch_prediction_array))\n",
        "\n",
        "    # If test_batch_size is not a divisor of num_test_images\n",
        "    if remain_images != 0:\n",
        "      self.test_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[remain_images, img_height, img_width, img_depth])\n",
        "      logits = resnet(self.test_image_placeholder, FLAGS.num_residual_blocks, reuse=True)\n",
        "      predictions = tf.nn.softmax(logits)\n",
        "      test_image_batch = test_image_array[-remain_images:, ...]\n",
        "      batch_prediction_array = sess.run(predictions, feed_dict={self.test_image_placeholder: test_image_batch})\n",
        "      prediction_array = np.concatenate((prediction_array, batch_prediction_array))\n",
        "\n",
        "    return prediction_array\n",
        "\n",
        "# Helper functions \n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "    labels = tf.cast(labels, tf.int64)\n",
        "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=labels)\n",
        "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
        "    return cross_entropy_mean\n",
        "\n",
        "  def error_acc(self, predictions, labels, k):\n",
        "    ''' Returns: top-1 error, accuracy'''\n",
        "    batch_size = predictions.get_shape().as_list()[0]\n",
        "    top1_precision = tf.nn.in_top_k(predictions, labels, k=1)\n",
        "    num_correct = tf.reduce_sum(tf.cast(top1_precision, tf.float32))\n",
        "    # self.train_accuracy = (num_correct) / float(batch_size)\n",
        "    return ((batch_size - num_correct) / float(batch_size)) , (num_correct) / float(batch_size)\n",
        "\n",
        "  def train_op(self, global_step, total_loss, top1_error,accuracy):\n",
        "    tf.summary.scalar('train_loss', total_loss)\n",
        "    tf.summary.scalar('train_top1_error', top1_error)\n",
        "    tf.summary.scalar('train_accuracy',accuracy)\n",
        "    \n",
        "    # Compute moving average of loss, error, accuracy\n",
        "    ema = tf.train.ExponentialMovingAverage(FLAGS.train_ema_decay, global_step)\n",
        "    train_ema_op = ema.apply([total_loss, top1_error,accuracy]) # Generates moving avg of error, loss, acc\n",
        "    tf.summary.scalar('train_top1_error_avg', ema.average(top1_error))\n",
        "    tf.summary.scalar('train_loss_avg', ema.average(total_loss))\n",
        "    tf.summary.scalar('train_accuracy_avg',ema.average(accuracy))\n",
        "\n",
        "    tf.summary.scalar('learning_rate', self.lr_ph)\n",
        "\n",
        "    # Define optimizer \n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate=self.lr_ph, momentum=0.9)\n",
        "    train_op = optimizer.minimize(total_loss, global_step=global_step)\n",
        "    return train_op, train_ema_op\n",
        "\n",
        "  def validation_op(self, validation_step, top1_error, loss,accuracy):\n",
        "    ema_0 = tf.train.ExponentialMovingAverage(0.0, validation_step) # Decay = 0.0 --> returns original error\n",
        "    ema = tf.train.ExponentialMovingAverage(0.95, validation_step) \n",
        "\n",
        "    val_op = tf.group(validation_step.assign_add(1), ema_0.apply([top1_error, loss]), ema.apply([top1_error, loss]))\n",
        "\n",
        "    val_top1_error = ema_0.average(top1_error)\n",
        "    val_loss = ema_0.average(loss)\n",
        "\n",
        "    top1_error_avg = ema.average(top1_error)\n",
        "    loss_val_avg = ema.average(loss)\n",
        "\n",
        "    # Add to tensorboard \n",
        "    tf.summary.scalar('val_top1_error', val_top1_error)\n",
        "    tf.summary.scalar('val_loss', val_loss)\n",
        "    tf.summary.scalar('val_top1_error_avg', top1_error_avg)\n",
        "    tf.summary.scalar('val_loss_avg', loss_val_avg)\n",
        "\n",
        "    tf.summary.scalar('val_accuracy',accuracy)\n",
        "\n",
        "    return val_op\n",
        "\n",
        "  def full_validation(self, loss, top1_error, session, valid_data, valid_labels, batch_data,batch_label):\n",
        "    num_val_batches = 10000 // FLAGS.validation_batch_size\n",
        "    order = np.random.choice(10000, num_val_batches * FLAGS.validation_batch_size)\n",
        "    valid_data_subset = valid_data[order, ...]\n",
        "    valid_labels_subset = valid_labels[order]\n",
        "\n",
        "    loss_list,error_list = list(), list()\n",
        "    # error_list = list()\n",
        "\n",
        "    for step in range(num_val_batches):\n",
        "      offset = step * FLAGS.validation_batch_size\n",
        "      # Feed values for placeholders using feed_dict \n",
        "      feed_dict = {self.image_ph: batch_data, self.label_ph: batch_label,\n",
        "                    self.valid_image_ph: valid_data_subset[offset:offset+FLAGS.validation_batch_size, ...],\n",
        "                    self.valid_label_ph: valid_labels_subset[offset:offset+FLAGS.validation_batch_size],\n",
        "                    self.lr_ph: FLAGS.init_lr}\n",
        "      loss_value, top1_error_value = session.run([loss, top1_error], feed_dict=feed_dict)\n",
        "      loss_list.append(loss_value)\n",
        "      error_list.append(top1_error_value)\n",
        "      \n",
        "    return np.mean(loss_list), np.mean(error_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_6QHxpKFjSZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "c079b536-cb33-4b07-8be2-16895af2db47"
      },
      "source": [
        "train = Train_ResNet()\n",
        "train.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read all data\n",
            "Shuffle\n",
            "Read all data\n",
            "Shuffle\n",
            "WARNING:tensorflow:From <ipython-input-9-1155bc89a28b>:4: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py:507: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "2020-05-05 21:30:02.690542: step 0, loss = 2.3279 (79.2 examples/sec; 1.616 sec/batch)\n",
            "Train top1 err =  0.9375\n",
            "Valid top1 err = 0.9200\n",
            "Valid loss =  2.3039937\n",
            "2020-05-05 21:37:54.942838: step 400, loss = 2.0131 (110.5 examples/sec; 1.159 sec/batch)\n",
            "Train top1 err =  0.515625\n",
            "Valid top1 err = 0.5480\n",
            "Valid loss =  2.006255\n",
            "2020-05-05 21:45:41.491856: step 800, loss = 1.9539 (110.3 examples/sec; 1.160 sec/batch)\n",
            "Train top1 err =  0.4609375\n",
            "Valid top1 err = 0.4040\n",
            "Valid loss =  1.8785331\n",
            "2020-05-05 21:53:28.613462: step 1200, loss = 1.8050 (112.2 examples/sec; 1.141 sec/batch)\n",
            "Train top1 err =  0.2890625\n",
            "Valid top1 err = 0.4280\n",
            "Valid loss =  1.8761128\n",
            "2020-05-05 22:01:15.503943: step 1600, loss = 1.8499 (112.9 examples/sec; 1.134 sec/batch)\n",
            "Train top1 err =  0.34375\n",
            "Valid top1 err = 0.3280\n",
            "Valid loss =  1.7936356\n",
            "2020-05-05 22:08:59.340015: step 2000, loss = 1.8361 (110.7 examples/sec; 1.157 sec/batch)\n",
            "Train top1 err =  0.3203125\n",
            "Valid top1 err = 0.3000\n",
            "Valid loss =  1.772109\n",
            "2020-05-05 22:16:37.314528: step 2400, loss = 1.7645 (112.7 examples/sec; 1.135 sec/batch)\n",
            "Train top1 err =  0.2421875\n",
            "Valid top1 err = 0.2680\n",
            "Valid loss =  1.7345986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujr1QHiGPEC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8cRGEs5JMio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nXAlQiFFc2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs_test_6"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}